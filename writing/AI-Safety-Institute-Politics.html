<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google-site-verification" content="tm5Y6ZNTf-lBqbwniGjQPv1q02o2TuUQZ9GTYa4SMLg" />
    <title>
        The AI Safety Institute Brings People Together — Don’t Let Politics Tear It Apart
    </title>
    <meta name="description"
        content="Henry Josephson's personal website. Crosswords, AI Policy, Philosophy" />
    <meta name="keywords" content="s" />
    <meta property="og:title" content="Henry Josephson" />
    <meta property="og:url" content="https://www.henryjosephson.com" />
    <meta property="og:description"
        content="Henry Josephson's personal website. Crosswords, AI Policy, Philosophy." />
    <meta property="og:type" content="website" />

    <link rel="stylesheet" href="https://latex.vercel.app/style.css">
    <link rel="stylesheet" href="https://latex.vercel.app/prism/prism.css">
    <link rel="stylesheet" href="/emails.css">
    <link rel="stylesheet" href="/js/header.css">
    <script src="/js/subscribe.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
</head>
<body id="top" class="latex-dark-auto">
    <div class="site-header">
        <a href="/">Home</a>
        <a href="/writing/index.html" class="active">Writing</a>
        <a href="/xw/index.html">Crosswords</a>
        <a href="/now/index.html">Now</a>
    </div>
    <header>
        <h1 id="title"><span class="latex">
            The AI Safety Institute Brings People Together — Don’t Let Politics Tear It Apart</h1>
        <p class="author">
        Henry Josephson<br>October 13, 2024
        </p>
    </header>
    <div class="subscribe-here"></div>
    <p>In his 2024 <a
    href="https://rncplatform.donaldjtrump.com/">platform</a>, sandwiched
    between sections on immigration and lowering healthcare costs, Donald
    Trump has vowed to repeal President Joe Biden’s <a
    href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">2023
    Executive Order on AI</a>.</p>
    <p>While largely symbolic — the <a
    href="https://www.dhs.gov/sites/default/files/2024-06/24_0620_cwmd-dhs-cbrn-ai-eo-report-04262024-public-release.pdf">reports</a>,
    <a href="https://nairrpilot.org/projects/awarded">research</a>, and <a
    href="https://tech.ed.gov/files/2024/07/Designing-for-Education-with-Artificial-Intelligence-An-Essential-Guide-for-Developers.pdf">guidance</a>
    that made up much of the executive order can’t be unwritten, and many
    large AI companies (including OpenAI, Meta, Anthropic, and Google) <a
    href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024">voluntarily
    committed</a> to sharing results of safety tests with the government —
    overturning the executive order would have one serious consequence:
    dissolving the US AI Safety Institute (AISI).</p>
    <p>Located in the National Institute of Standards and Technology, AISI’s
    job is to balance AI innovation with safety concerns, creating a safe,
    business-friendly environment that doesn’t stifle progress.</p>
    <p>It’s the hub of the government’s cutting-edge research and for
    experts, stakeholders, and countries to collaborate on AI safety
    initiatives. It <a
    href="https://www.nist.gov/ai-test-evaluation-validation-and-verification-tevv">sets
    non-binding standards for AI safety</a>, ensuring that everybody in the
    industry is on the same page. It <a
    href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.800-1.ipd.pdf">provides
    technical guidance</a>, helping AI companies protect their intellectual
    property and AI model weights from theft.</p>
    <p>And perhaps most crucially, the Institute is the only organization
    that <a
    href="https://www.theverge.com/2024/8/29/24231395/openai-anthropic-share-models-us-ai-safety-institute">actively
    evaluates next-generation AI models prior to deployment.</a>This access
    goes far beyond the results of certain safety tests that I mentioned
    above: before AISI’s deal with cutting-edge AI companies, there <a
    href="https://ailabwatch.org/blog/external-evaluation/">was</a> “only
    one recent instance of a lab giving access to an [external] evaluator
    pre-deployment.” But now researchers have unprecedented access to
    developing AI technology. This new level of access is a game-changer —
    akin to the difference between evaluating a car prototype on its crash
    test data and driving itself yourself.</p>
    <p>Regardless of where you fall on the spectrum between
    “superintelligent AI will turn us all into paperclips” and “AI is just
    math,” there are strong reasons to support the AISI. If you care about
    <a
    href="https://aclanthology.org/2023.acl-long.656.pdf">anti-conservative
    bias</a>, <a
    href="https://www.nytimes.com/2024/02/22/technology/google-gemini-german-uniforms.html">anti-Black
    bias</a>, or AI being used to <a
    href="https://www.firstpost.com/tech/openais-o1-model-aka-strawberry-can-create-bioweapons-comes-with-medium-risk-accepts-ai-giant-13815749.html">create
    bioweapons</a>, letting outside experts evaluate something so
    consequential is just common sense.</p>
    <p>Dissolving AISI means losing pre-deployment access. It means risk
    blindly releasing potentially harmful AI systems into society. This will
    undermine public trust in new AI technologies and safety — to say
    nothing of more direct harms.</p>
    <p>So what can we do about it?</p>
    <p>Leaving aside your vote in November, there are two bipartisan bills
    that need your urgent support. There’s one in the <a
    href="https://www.congress.gov/bill/118th-congress/house-bill/9497/text">House</a>
    and one in the <a
    href="https://www.congress.gov/bill/118th-congress/senate-bill/4178/">Senate</a>,
    and each would codify AISI into law, protecting it from removal via
    executive order. If you think the AISI is valuable, write to your
    congressman. Call your Senator. Do your part to pass these bills.</p>
    <p>The time is now. The AI landscape is evolving rapidly, and without
    the AISI, the U.S.’s safety efforts will fall behind. Something we — and
    the rest of the world — can’t afford.</p>
    <p>If the above bills don’t pass, the future of the AI Safety Institute
    will be on the presidential ballot.</p>
    <p>We can either lead in AI safety or be led by countries who may not
    share our values or priorities.</p>
    <p>Let’s choose leadership. Let’s choose safety. Let’s choose to keep
    the AI Safety Institute intact and empower it to secure our AI-driven
    future.</p>
</body>