<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="google-site-verification" content="tm5Y6ZNTf-lBqbwniGjQPv1q02o2TuUQZ9GTYa4SMLg" />
    <title>
        The AI Safety Institute Brings People Together — Don’t Let Politics Tear It Apart
    </title>
    <meta name="description"
        content="Henry Josephson's personal website. Crosswords, AI Policy, Philosophy" />
    <meta name="keywords" content="s" />
    <meta property="og:title" content="Henry Josephson" />
    <meta property="og:url" content="https://www.henryjosephson.com" />
    <meta property="og:description"
        content="Henry Josephson's personal website. Crosswords, AI Policy, Philosophy." />
    <meta property="og:type" content="website" />

    <link rel="stylesheet" href="https://latex.vercel.app/style.css">
    <link rel="stylesheet" href="https://latex.vercel.app/prism/prism.css">
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
</head>
<body id="top" class="latex-dark-auto">
    <div style="text-align: center">
        <a href="../index.html">henryjosephson.com</a>
    </div>
    <header>
        <h1 id="title"><span class="latex"></span>
            NY Should Pass Light-Touch Law to Keep AI Innovation Responsible
        </h1>
        <p class="author">
        Henry Josephson<br>
        October 13, 2024
        </p>
    </header>
        
    <strong>Or “Alex Bores Should Introduce SB1047 In NY”</strong></p>
    <p><em>Author’s note: Since writing this, I’ve read <a
    href="https://www.astralcodexten.com/p/sb-1047-our-side-of-the-story">Scott
    Alexander’s SB1047 postmortem</a>, which has made me think that the DSA
    / left wing of New York’s democratic party might be more amenable to
    this sort of bill than I initially thought.</em><a href="#fn1"
    class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
    <p>The biggest AI policy news of the past week has been Gavin Newsom’s
    veto of SB1047. Especially in an era of concern about <a
    href="https://www.nytimes.com/2024/09/16/business/china-ai-safety.html">catastrophic
    risks from advanced AI</a>, SB1047 was CA State Senator Scott Weiner’s
    attempt at light-touch regulation which would make sure that the
    corporations pushing the frontier do it responsibly.</p>
    <p>Because I want to keep this piece short and because I think my
    audience is sympathetic, I won’t argue why I think some sort of frontier
    AI regulation would be good.<a href="#fn2" class="footnote-ref"
    id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
    <p><a href="https://thezvi.substack.com/p/guide-to-sb-1047">Here</a>’s a
    quick summary of what the bill would have done:<a href="#fn3"
    class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
    <blockquote>
    <p>IF AND ONLY IF you wish to train a model using $100 million or more
    in compute (including your fine-tuning costs):</p>
    </blockquote>
    <blockquote>
    <p>You must create a reasonable safety and security plan (SSP) such that
    your model does not pose an unreasonable risk of causing or materially
    enabling critical harm: mass casualties or incidents causing $500
    million or more in damages.</p>
    </blockquote>
    <blockquote>
    <p>That SSP must explain what you will do, how you will do it, and why.
    It must have objective evaluation criteria for determining compliance.
    It must include cybersecurity protocols to prevent the model from being
    unintentionally stolen.</p>
    </blockquote>
    <blockquote>
    <p>You must publish a redacted copy of your SSP, an assessment of the
    risk of catastrophic harms from your model, and get a yearly audit.</p>
    </blockquote>
    <blockquote>
    <p>You must adhere to your own SSP and publish the results of your
    safety tests.</p>
    </blockquote>
    <blockquote>
    <p>You must be able to shut down all copies under your control, if
    necessary.</p>
    </blockquote>
    <blockquote>
    <p>The quality of your SSP and whether you followed it will be
    considered in whether you used reasonable care.</p>
    </blockquote>
    <blockquote>
    <p>If you violate these rules, you do not use reasonable care and harm
    results, the Attorney General can fine you in proportion to training
    costs, plus damages for the actual harm.</p>
    </blockquote>
    <blockquote>
    <p>If you fail to take reasonable care, injunctive relief can be sought.
    The quality of your SSP, and whether or not you complied with it, shall
    be considered when asking whether you acted reasonably.</p>
    </blockquote>
    <blockquote>
    <p>Fine-tunes that spend $10 million or more are the responsibility of
    the fine-tuner.</p>
    </blockquote>
    <blockquote>
    <p>Fine-tunes spending less than that are the responsibility of the
    original developer.</p>
    </blockquote>
    <p>The bill is dead in California,<a href="#fn4" class="footnote-ref"
    id="fnref4" role="doc-noteref"><sup>4</sup></a> but we could bring it
    back to life in New York. In the rest of this piece, I’ll briefly
    explore:</p>
    <ol type="1">
    <li><p>Why here and now?</p></li>
    <li><p>What would have to change in the bill?</p></li>
    </ol>
    <h3 id="why-new-york">Why New York?</h3>
    <p>New York passes legislation. We introduced more bills than any other
    state last year, and enacted the third-most.<a href="#fn5"
    class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
    <p>More specifically (as you well know, you’re quoted in the article),
    the New York legislature is already <a
    href="http://entral-ny/politics/2024/10/03/new-york-lawmakers-on-drafting-new-ai-regulations">thinking
    a lot about AI</a>. Senator Kristen Gonzalez’s <a
    href="https://spectrumlocalnews.com/nys/central-ny/news/2024/07/31/sen--kristen-gonzalez-on-her-bill-to-oversee-a-i--in-state-government">bill</a>
    from last session was just the start: there are at least 40 AI bills in
    assembly committees right now,<a href="#fn6" class="footnote-ref"
    id="fnref6" role="doc-noteref"><sup>6</sup></a> which means there’s
    motivation to get this done.</p>
    <p>New York isn’t just <a
    href="https://www.osc.ny.gov/files/reports/osdc/pdf/report-10-2023.pdf">home
    to 5.7% of the nation’s tech jobs</a>, behind only California and Texas
    — it’s also an economic powerhouse all to itself. 52 of this year’s
    Fortune 500 companies <a
    href="https://www.visualcapitalist.com/map-the-number-of-fortune-500-companies-in-each-u-s-state/">were
    based in NY</a>, second only to — you guessed it — California.</p>
    <p>New York is in a great spot to act: strike while the iron is still
    hot — we’re in session right now, people want to pass AI laws, and we
    can use SB1047 while it’s fresh in everyone’s minds as both inspiration
    and a punching bag to get NY’s bill through (more on that second part
    below).</p>
    <p>Most importantly, if we craft the law correctly it can <em>de
    facto</em> apply outside of NY. This is something that basically only CA
    or NY could easily do, and the <em>de facto</em> national effects
    would’ve been one of SB1047’s biggest benefits.</p>
    <p>The rough idea is as follows: New York is a giant market. If a
    company has to choose between complying with light-touch regulation or
    being cutting off from every company in New York, they’ll probably
    choose the former. Crucially, though, if we craft the regulation just
    right and pick a target that’s tough to edit in a few key ways, it can
    be cheaper to just use the NY-compliant version everywhere — even the
    places that don’t require it.</p>
    <p>The <a
    href="https://archive.org/details/tradingupconsume0000voge">textbook
    example</a> of this is California’s auto emissions standards. In the
    ’90s, the EPA let CA set emissions laws that were stricter than the rest
    of the country’s. It turned out that, in many cases, it was just cheaper
    for car manufacturers to sell a CA compliant car everywhere than it was
    for them to make two versions.</p>
    <p>This is also part of what makes a new CA animal welfare law so
    potent: <a
    href="https://en.wikipedia.org/wiki/2018_California_Proposition_12">Prop
    12</a>, as it’s called, affects any pork that’s sold in California, no
    matter where it’s from. Producers in other states need to follow CA’s
    rules if they want access to CA’s massive market. Though the National
    Pork Producers Council argued it was unconstitutional regulation of
    interstate commerce, the Supreme Court ruled it <a
    href="https://www.vox.com/future-perfect/23721488/prop-12-scotus-pork-pigs-factory-farming-california-bacon">kosher</a>.</p>
    <p>As I’ve <a
    href="https://www.henryjosephson.com/AI-CA-Effect-2022.pdf">written
    before</a>, AI is a particularly good candidate for this sort of effect
    —training runs are very expensive, so it’d be tough to do two separate
    mega-training runs — one for NY, with a safety plan, and one for the
    rest of the world without one — just to dodge the bill. You could stop
    training early and release a half-baked model in NY, but you’d then be
    running a similar risk as you’d run if you just stopped selling your
    model in New York at all: every company in New York City using one of
    your competitors.</p>
    <p>What’s more, the ubiquity of the regulation is important for another
    reason: it means that companies in NY can’t avoid the regulation by
    leaving. This is important, since one of the biggest critiques levied
    against SB1047 was that it would drive tech innovation of of
    California<a href="#fn7" class="footnote-ref" id="fnref7"
    role="doc-noteref"><sup>7</sup></a> — preventing covered models from
    being sold in NY, no matter where they were trained, avoids this
    problem.</p>
    <p>Let’s explore some other changes we might need to make to SB1047
    before we pass a similar bill in NY.</p>
    <h3 id="what-needs-to-change-about-the-bill">What needs to change about
    the bill?</h3>
    <p>One of the biggest reasons to move on SB1047-esque legislation now is
    that we can improve on the ways it failed. Learning opportunities at
    this scale are rare, and the farther SB1047 gets into the rear-view
    mirror, the less applicable its lessons will be.</p>
    <p>Some changes are more substantive — we can probably lose the section
    establishing the CalCompute cluster (especially with the plans for the
    NY cluster proceeding separately), and saving the compute cluster KYC
    for another bill might streamline things.</p>
    <p>Some changes, though, are cosmetic — a large amount of opposition to
    SB1047 was based on misunderstandings of what the bill would actually
    do.</p>
    <p>For one, many were concerned that the bill would <a
    href="https://static.politico.com/95/0a/a317efe44616af436ce6a4f32647/founder-led-statement-on-sb1047-june-20-2024-2.pdf">disproportionately
    impact startups</a>, when the bill only applied to training runs costing
    more than $100 million.<a href="#fn8" class="footnote-ref" id="fnref8"
    role="doc-noteref"><sup>8</sup></a> It’s tough to overstate how big that
    is — See fig. 2 <a
    href="https://epochai.org/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year">here</a>
    — no model we know of has reached the 10^24 FLOP threshold.</p>
    <p>The<a
    href="https://aventis-advisors.com/ai-valuation-multiples/"> median
    series C round for an AI startup in 2024 is $54 million</a>. Even if
    startups were spending every cent of cash they had on hand on training a
    new foundation model, none but the largest would be affected by the
    bill.</p>
    <p>SB1047 would have also covered fine-tuning runs over $10 million —
    would this have crushed startups? Well, <a
    href="https://a16z.com/generative-ai-enterprise-2024/">Andreesen
    Horowitz estimated earlier this year</a> that “the average spend across
    foundation model APIs, self-hosting, and fine-tuning models was $7M
    across the dozens of companies we spoke to.” Even if we assume that all
    the money was going towards a single fine-tuning run, that fine-tuning
    run still wouldn’t be affected.</p>
    <p>You could very well reply (as I imagine a16z would), that these
    numbers are only going to go up, and that many startups could be
    spending over $10 million on single fine-tuning runs within a year or
    so. These are valid concerns! That’s why the bill would create a group
    of experts who can raise the threshold but can’t lower it. The point of
    the bill isn’t to make things harder for startups, and it’d be trivial
    to include a line affirming that the intent is to make sure the frontier
    is pushed responsibly, not to stop your Y-Combinator-backed GPT-4
    wrapper.</p>
    <p>There are a few more examples — e.g. the worry that the “kill switch”
    requirement would <em>de facto</em> kill open source models<a
    href="#fn9" class="footnote-ref" id="fnref9"
    role="doc-noteref"><sup>9</sup></a> and the worry that the liability was
    too broad.<a href="#fn10" class="footnote-ref" id="fnref10"
    role="doc-noteref"><sup>10</sup></a></p>
    <p>I see these all as PR failures more than legislative failures, and I
    don’t think it’d be too difficult to speak as if you opposed SB1047’s
    supposed suppressive effects on startups and open-source models — that’s
    why <em>our</em> bill only applies to closed-source and big tech
    training runs that cost more than the GDP of a micronation.</p>
    <p>To <a
    href="https://thezvi.substack.com/p/guide-to-sb-1047">paraphrase
    Zvi</a>, there’s only one scenario in which AI gets slowed down by a
    bill like SB1047:</p>
    <ol type="1">
    <li><p>The model in question is bigger than one we’ve ever seen
    before,</p></li>
    <li><p>It would have been released without the bill,</p></li>
    <li><p>But its developers didn’t take reasonable care — or even have a
    plan — to reduce the risk that their model causes mass casualties or
    &gt;$500 million in damages.</p></li>
    </ol>
    <p>And you know what? If a developer is unwilling or unable to take
    reasonable care to stop their model from being the but-for cause of mass
    casualties or &gt;$500 million in damages? That’s the sort of developer
    I’d want to stop.</p>
    <p>We have an opportunity, right now, to make sure that capabilities
    advance responsibly without stifling innovation, harming startups, or
    disincentivizing open-source development. We can address misconceptions,
    clarify intent, and maintain focus on keeping the largest models
    responsible.</p>
    <p>The time to act is now, while the lessons from California are fresh
    and the need for thoughtful AI governance is clear.</p>
    <!-- Footnotes themselves at the bottom. -->
    <h2 id="notes">Notes</h2>
    <section id="footnotes" class="footnotes footnotes-end-of-document"
    role="doc-endnotes">
    <hr />
    <ol>
    <li id="fn1"><p>More specifically, Alexander writes about a sense that
    SB1047 was the best chance the AI industry would get at light-touch
    regulation, and that when the next bill comes from anti-big-tech folks
    further to the left than Sen. Weiner, the tech companies that opposed
    SB1047 will have to lie in the bed they’ve made. Excerpts from Jacobin
    and Current Affairs seem to back up that socialists are genuinely
    interested in this, and give a sort of “us vs. the capitalists who don’t
    care about safety” vibe that makes me (an Albany outsider) think that
    DSA support might be more feasible than I’d initially think.<a
    href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn2"><p>That’s another blogpost that’s in the works, though!<a
    href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn3"><p>It’s from Zvi Moshowitz (who lives in Brooklyn!).<a
    href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn4"><p>Unless the CA legislature overrides Newsom’s veto, <a
    href="https://calmatters.org/politics/2024/10/californa-veto-overrides/">though
    they haven’t overturned a veto since 1979</a>.<a href="#fnref4"
    class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn5"><p>You probably wouldn’t guess the first two — Texas in
    first place, then Tennessee in second. <a
    href="https://fiscalnote-marketing.s3.amazonaws.com/FN080823-Most-Effective-States-WP_v2.pdf">Source</a><a
    href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn6"><p>This is the count of 2024 bills with the string
    “artificial intelligence” in their title. This is probably an
    underestimate, since some AI bills probably don’t say “AI” in the title.
    If you want to check this yourself, I have a good script to download a
    json of all current NY bills <a
    href="https://github.com/henryjosephson/State-Legislator-Effectiveness-Dashboard/blob/main/src/state_specific_data_downloads/NY_read_senate_api.py">here</a>,
    or you can use <a href="http://legiscan.com/datasets">Legiscan’s bulk
    download tool</a>.<a href="#fnref6" class="footnote-back"
    role="doc-backlink">↩︎</a></p></li>
    <li id="fn7"><p>To be clear, I don’t think SB1047 would have caused
    companies to leave CA in this way, but others did.<a href="#fnref7"
    class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn8"><p>It also applied to fine-tuning runs that cost &gt;$10
    million. I cover this two paragraphs later.<a href="#fnref8"
    class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    <li id="fn9"><p>Consider the worries that the requirement for a “kill
    switch” would <em>de facto</em> kill open source models. The original
    SB1047 only applied this to instances “controlled by a developer” —
    i.e. not the open-weights version I’m running on my laptop — but the
    fact that Garry Tan wrote <a
    href="https://static.politico.com/95/0a/a317efe44616af436ce6a4f32647/founder-led-statement-on-sb1047-june-20-2024-2.pdf">a
    letter</a> contending that “the requirement of a Kill Switch is possibly
    a backdoor open source AI ban” means this point wasn’t clear enough.
    It’d be easy to say “SB1047 sucked because it could have backdoored a
    ban of open-weights AI. We think that’s bad! That’s why no portion of
    this bill should be interpreted as backdooring a ban on open-weights or
    open-source AI.”<a href="#fnref9" class="footnote-back"
    role="doc-backlink">↩︎</a></p></li>
    <li id="fn10"><p>The same thing goes with the worry that an open-source
    “<a href="https://x.com/krishnanrohit/status/1824559133904998619">model
    developer is liable if the hackers use [their model] to hack the power
    grid</a>” — the liability only kicks in if the harm materially stems
    from the developer’s failure to take reasonable care. I think this was
    pretty clear in the original text of the bill, but it’s once again easy
    to say “you aren’t liable if someone uses your model to do harm in a
    crazy way you couldn’t reasonably have foreseen,” and we can easily
    codify this in the intent of the bill. The point isn’t to sue VSCode for
    making it easier to commit cyberattacks, it’s to sue models that
    counterfactually enable harms that wouldn’t happen without the model.”<a
    href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
    </ol>
    </section>
